package org.hammerlab.bam.check

import caseapp.{ Recurse, ExtraName ⇒ O, HelpMessage ⇒ M }
import org.apache.log4j.Level.WARN
import org.apache.log4j.Logger.getRootLogger
import org.apache.spark.rdd.RDD
import org.apache.spark.util.LongAccumulator
import org.hammerlab.app.{ SparkPathApp, SparkPathAppArgs }
import org.hammerlab.args.{ LogArgs, OutputArgs }
import org.hammerlab.bam.check.Checker.MakeChecker
import org.hammerlab.bam.check.indexed.IndexedRecordPositions
import org.hammerlab.bam.header.Header
import org.hammerlab.bam.kryo.Registrar
import org.hammerlab.bgzf.Pos
import org.hammerlab.bgzf.block.PosIterator
import org.hammerlab.channel.CachingChannel._
import org.hammerlab.channel.SeekableByteChannel
import org.hammerlab.iterator.FinishingIterator._
import org.hammerlab.paths.Path

/**
 * CLI for [[Main]]: check every (bgzf-decompressed) byte-position in a BAM file for a record-start with and compare the results to the true
 * read-start positions.
 *
 * - Takes one argument: the path to a BAM file.
 * - Requires that BAM to have been indexed prior to running by [[org.hammerlab.bgzf.index.IndexBlocks]] and
 *   [[org.hammerlab.bam.index.IndexRecords]].
 *
 * @param eager if set, run the [[org.hammerlab.bam.check.eager.Checker]] on the input BAM file. If both [[eager]] and
 *              [[seqdoop]] are set, they are compared to each other; if only one is set, then an
 *              [[IndexedRecordPositions.Args.records indexed-records]] file is assumed to exist for the BAM, and is
 *              used as the source of truth against which to compare.
 * @param seqdoop if set, run the [[org.hammerlab.bam.check.seqdoop.Checker]] on the input BAM file. If both [[eager]]
 *                and [[seqdoop]] are set, they are compared to each other; if only one is set, then an
 *                [[IndexedRecordPositions.Args.records indexed-records]] file is assumed to exist for the BAM, and is
 *                used as the source of truth against which to compare.
 */
case class Args(
  @Recurse blocks: Blocks.Args,
  @Recurse records: IndexedRecordPositions.Args,
  @Recurse output: OutputArgs,
  @Recurse logging: LogArgs,

  @O("e")
  @M("When set, run the \"eager\" checker, either against the \"seqdoop\" checker (if the --seqdoop / -s flag is passed), or against a ground truth indicated by a .records file generated by index-records. Passing neither the eager nor seqdoop flags has the same effect as passing both: they are run against each other")
  eager: Boolean = false,

  @O("q")
  @M("After running eager and/or seqdoop checkers over a BAM file and filtering to just the contested positions, repartition to have this many records per partition. Typically there are far fewer records at this stage, so it's useful to coalesce down to avoid 1,000's of empty partitions")
  resultsPerPartition: Int = 1000000,

  @O("s")
  @M("When set, run the \"seqdoop\" checker, either against the \"eager\" checker (if the --eager / -e flag is passed), or against a ground truth indicated by a .records file generated by index-records. Passing neither the eager nor seqdoop flags has the same effect as passing both: they are run against each other")
  seqdoop: Boolean = false
)
  extends SparkPathAppArgs

object Main
  extends SparkPathApp[Args](classOf[Registrar])
    with AnalyzeCalls {

  override def run(args: Args): Unit = {

    if (args.logging.warn)
      getRootLogger.setLevel(WARN)

    implicit val blockArgs = args.blocks
    implicit val indexedRecordArgs = args.records

    val header = Header(path)
    implicit val headerBroadcast = sc.broadcast(header)
    implicit val contigLengthsBroadcast = sc.broadcast(header.contigLengths)
    implicit val rangesBroadcast = sc.broadcast(args.blocks.ranges)

    val (compressedSizeAccumulator, calls) =
      (args.eager, args.seqdoop) match {
        case (true, false) ⇒
          vsIndexed[Boolean, eager.Checker]
        case (false, true) ⇒
          vsIndexed[Boolean, seqdoop.Checker]
        case _ ⇒
          compare[
            eager.Checker,
            seqdoop.Checker
          ]
      }

    analyzeCalls(
      calls,
      args.resultsPerPartition,
      compressedSizeAccumulator
    )
  }

  def compare[C1 <: Checker[Boolean], C2 <: Checker[Boolean]](
      implicit
      path: Path,
      args: Blocks.Args,
      makeChecker1: MakeChecker[Boolean, C1],
      makeChecker2: MakeChecker[Boolean, C2]
  ): (LongAccumulator, RDD[(Pos, (Boolean, Boolean))]) = {

    val blocks = Blocks()

    val compressedSizeAccumulator = sc.longAccumulator("compressedSizeAccumulator")

    val calls =
      blocks
        .mapPartitions {
          blocks ⇒
            val ch = SeekableByteChannel(path).cache
            val checker1 = makeChecker1(ch)
            val checker2 = makeChecker2(ch)

            blocks
              .flatMap {
                block ⇒
                  compressedSizeAccumulator.add(block.compressedSize)
                  PosIterator(block)
              }
              .map {
                pos ⇒
                  pos →
                    (
                      checker1(pos),
                      checker2(pos)
                    )
              }
              .finish(ch.close())
        }

    (
      compressedSizeAccumulator,
      calls
    )
  }
}
