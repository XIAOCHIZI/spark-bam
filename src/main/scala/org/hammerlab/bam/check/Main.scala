package org.hammerlab.bam.check

import caseapp.{ AppName, ProgName, Recurse, ExtraName ⇒ O, HelpMessage ⇒ M }
import org.apache.spark.rdd.RDD
import org.apache.spark.util.LongAccumulator
import org.hammerlab.app.{ SparkPathApp, SparkPathAppArgs }
import org.hammerlab.args.{ LogArgs, OutputArgs, PostPartitionArgs }
import org.hammerlab.bam.check.Checker.MakeChecker
import org.hammerlab.bam.check.indexed.IndexedRecordPositions
import org.hammerlab.bam.kryo.Registrar
import org.hammerlab.bgzf.Pos
import org.hammerlab.bgzf.block.PosIterator
import org.hammerlab.channel.CachingChannel._
import org.hammerlab.channel.SeekableByteChannel
import org.hammerlab.iterator.FinishingIterator._
import org.hammerlab.paths.Path

/**
 * CLI for [[Main]]: check every (bgzf-decompressed) byte-position in a BAM file for a record-start with and compare the results to the true
 * read-start positions.
 *
 * - Takes one argument: the path to a BAM file.
 * - Requires that BAM to have been indexed prior to running by [[org.hammerlab.bgzf.index.IndexBlocks]] and
 *   [[org.hammerlab.bam.index.IndexRecords]].
 *
 * @param eager if set, run the [[org.hammerlab.bam.check.eager.Checker]] on the input BAM file. If both [[eager]] and
 *              [[seqdoop]] are set, they are compared to each other; if only one is set, then an
 *              [[IndexedRecordPositions.Args.records indexed-records]] file is assumed to exist for the BAM, and is
 *              used as the source of truth against which to compare.
 * @param seqdoop if set, run the [[org.hammerlab.bam.check.seqdoop.Checker]] on the input BAM file. If both [[eager]]
 *                and [[seqdoop]] are set, they are compared to each other; if only one is set, then an
 *                [[IndexedRecordPositions.Args.records indexed-records]] file is assumed to exist for the BAM, and is
 *                used as the source of truth against which to compare.
 */
@AppName("Check all uncompressed positions in a BAM file for record-boundary-identification errors")
@ProgName("… org.hammerlab.bam.check.Main")
case class Args(
  @Recurse blocks: Blocks.Args,
  @Recurse records: IndexedRecordPositions.Args,
  @Recurse logging: LogArgs,
  @Recurse output: OutputArgs,
  @Recurse partitioning: PostPartitionArgs,

  @O("e")
  @M("When set, run the \"eager\" checker, either against the \"seqdoop\" checker (if the --seqdoop / -s flag is passed), or against a ground truth indicated by a .records file generated by index-records. Passing neither the eager nor seqdoop flags has the same effect as passing both: they are run against each other")
  eager: Boolean = false,

  @O("s")
  @M("When set, run the \"seqdoop\" checker, either against the \"eager\" checker (if the --eager / -e flag is passed), or against a ground truth indicated by a .records file generated by index-records. Passing neither the eager nor seqdoop flags has the same effect as passing both: they are run against each other")
  seqdoop: Boolean = false
)
  extends SparkPathAppArgs

object Main
  extends SparkPathApp[Args](classOf[Registrar])
    with AnalyzeCalls {

  override def run(args: Args): Unit = {

    new CheckerMain(args) {
      override def run(): Unit = {

        val (compressedSizeAccumulator, calls) =
          (args.eager, args.seqdoop) match {
            case (true, false) ⇒
              vsIndexed[Boolean, eager.Checker]
            case (false, true) ⇒
              vsIndexed[Boolean, seqdoop.Checker]
            case _ ⇒
              compare[
                eager.Checker,
                seqdoop.Checker
                ]
          }

        analyzeCalls(
          calls,
          args.partitioning.resultsPerPartition,
          compressedSizeAccumulator
        )
      }
    }
  }

  def compare[C1 <: Checker[Boolean], C2 <: Checker[Boolean]](
      implicit
      path: Path,
      args: Blocks.Args,
      makeChecker1: MakeChecker[Boolean, C1],
      makeChecker2: MakeChecker[Boolean, C2]
  ): (LongAccumulator, RDD[(Pos, (Boolean, Boolean))]) = {

    val blocks = Blocks()

    val compressedSizeAccumulator = sc.longAccumulator("compressedSizeAccumulator")

    val calls =
      blocks
        .mapPartitions {
          blocks ⇒
            val ch = SeekableByteChannel(path).cache
            val checker1 = makeChecker1(ch)
            val checker2 = makeChecker2(ch)

            blocks
              .flatMap {
                block ⇒
                  compressedSizeAccumulator.add(block.compressedSize)
                  PosIterator(block)
              }
              .map {
                pos ⇒
                  pos →
                    (
                      checker1(pos),
                      checker2(pos)
                    )
              }
              .finish(ch.close())
        }

    (
      compressedSizeAccumulator,
      calls
    )
  }
}
